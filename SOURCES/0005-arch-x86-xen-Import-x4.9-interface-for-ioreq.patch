From ba1e7d657cc0991cfd53558e00043e9e1d080a73 Mon Sep 17 00:00:00 2001
From: Xiong Zhang <xiong.y.zhang@intel.com>
Date: Wed, 19 Apr 2017 05:17:16 +0800
Subject: [PATCH 05/45] arch/x86/xen: Import x4.9 interface for ioreq

Signed-off-by: Xiong Zhang <xiong.y.zhang@intel.com>
---
 drivers/xen/ioemu.c                |  21 ---
 include/xen/interface/hvm/dm_op.h  | 369 ++++++++++++++++++++++++++++++++++++-
 include/xen/interface/hvm/hvm_op.h | 363 ++++++++++++++++++------------------
 include/xen/interface/hvm/ioreq.h  | 112 +++++------
 4 files changed, 613 insertions(+), 252 deletions(-)

diff --git a/drivers/xen/ioemu.c b/drivers/xen/ioemu.c
index e9a8022..341b4a4 100644
--- a/drivers/xen/ioemu.c
+++ b/drivers/xen/ioemu.c
@@ -36,27 +36,6 @@
 
 #include <asm/xen/hypercall.h>
 
-/*
- * XEN_DMOP_inject_msi: Inject an MSI for an emulated device.
- */
-#define XEN_DMOP_inject_msi 14
-
-struct xen_dm_op_inject_msi {
-	/* IN - MSI data (lower 32 bits) */
-	uint32_t data;
-	uint32_t pad;
-	/* IN - MSI address (0xfeexxxxx) */
-	uint64_t __attribute__((aligned(8))) addr;
-};
-
-struct xen_dm_op {
-	uint32_t op;
-	uint32_t pad;
-	union {
-		struct xen_dm_op_inject_msi inject_msi;
-	} u;
-};
-
 /**
  * xen_ioemu_inject_msi - inject an MSI into a guest
  * @domid: domain to inject MSI into
diff --git a/include/xen/interface/hvm/dm_op.h b/include/xen/interface/hvm/dm_op.h
index ee9e480..556d0db 100644
--- a/include/xen/interface/hvm/dm_op.h
+++ b/include/xen/interface/hvm/dm_op.h
@@ -18,15 +18,382 @@
  * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
  * DEALINGS IN THE SOFTWARE.
+ *
  */
 
 #ifndef __XEN_PUBLIC_HVM_DM_OP_H__
 #define __XEN_PUBLIC_HVM_DM_OP_H__
 
+#include "../xen.h"
+
+#include "../event_channel.h"
+
+#ifndef uint64_aligned_t
+#define uint64_aligned_t uint64_t
+#endif
+
+/*
+ * IOREQ Servers
+ *
+ * The interface between an I/O emulator an Xen is called an IOREQ Server.
+ * A domain supports a single 'legacy' IOREQ Server which is instantiated if
+ * parameter...
+ *
+ * HVM_PARAM_IOREQ_PFN is read (to get the gmfn containing the synchronous
+ * ioreq structures), or...
+ * HVM_PARAM_BUFIOREQ_PFN is read (to get the gmfn containing the buffered
+ * ioreq ring), or...
+ * HVM_PARAM_BUFIOREQ_EVTCHN is read (to get the event channel that Xen uses
+ * to request buffered I/O emulation).
+ *
+ * The following hypercalls facilitate the creation of IOREQ Servers for
+ * 'secondary' emulators which are invoked to implement port I/O, memory, or
+ * PCI config space ranges which they explicitly register.
+ */
+
+typedef uint16_t ioservid_t;
+
+/*
+ * XEN_DMOP_create_ioreq_server: Instantiate a new IOREQ Server for a
+ *                               secondary emulator.
+ *
+ * The <id> handed back is unique for target domain. The valur of
+ * <handle_bufioreq> should be one of HVM_IOREQSRV_BUFIOREQ_* defined in
+ * hvm_op.h. If the value is HVM_IOREQSRV_BUFIOREQ_OFF then  the buffered
+ * ioreq ring will not be allocated and hence all emulation requests to
+ * this server will be synchronous.
+ */
+#define XEN_DMOP_create_ioreq_server 1
+
+struct xen_dm_op_create_ioreq_server {
+	/* IN - should server handle buffered ioreqs */
+	uint8_t handle_bufioreq;
+	uint8_t pad[3];
+	/* OUT - server id */
+	ioservid_t id;
+};
+
+/*
+ * XEN_DMOP_get_ioreq_server_info: Get all the information necessary to
+ *                                 access IOREQ Server <id>.
+ *
+ * The emulator needs to map the synchronous ioreq structures and buffered
+ * ioreq ring (if it exists) that Xen uses to request emulation. These are
+ * hosted in the target domain's gmfns <ioreq_pfn> and <bufioreq_pfn>
+ * respectively. In addition, if the IOREQ Server is handling buffered
+ * emulation requests, the emulator needs to bind to event channel
+ * <bufioreq_port> to listen for them. (The event channels used for
+ * synchronous emulation requests are specified in the per-CPU ioreq
+ * structures in <ioreq_pfn>).
+ * If the IOREQ Server is not handling buffered emulation requests then the
+ * values handed back in <bufioreq_pfn> and <bufioreq_port> will both be 0.
+ */
+#define XEN_DMOP_get_ioreq_server_info 2
+
+struct xen_dm_op_get_ioreq_server_info {
+	/* IN - server id */
+	ioservid_t id;
+	uint16_t pad;
+	/* OUT - buffered ioreq port */
+	evtchn_port_t bufioreq_port;
+	/* OUT - sync ioreq pfn */
+	uint64_aligned_t ioreq_pfn;
+	/* OUT - buffered ioreq pfn */
+	uint64_aligned_t bufioreq_pfn;
+};
+
+/*
+ * XEN_DMOP_map_io_range_to_ioreq_server: Register an I/O range for
+ *                                        emulation by the client of
+ *                                        IOREQ Server <id>.
+ * XEN_DMOP_unmap_io_range_from_ioreq_server: Deregister an I/O range
+ *                                            previously registered for
+ *                                            emulation by the client of
+ *                                            IOREQ Server <id>.
+ *
+ * There are three types of I/O that can be emulated: port I/O, memory
+ * accesses and PCI config space accesses. The <type> field denotes which
+ * type of range* the <start> and <end> (inclusive) fields are specifying.
+ * PCI config space ranges are specified by segment/bus/device/function
+ * values which should be encoded using the DMOP_PCI_SBDF helper macro
+ * below.
+ *
+ * NOTE: unless an emulation request falls entirely within a range mapped
+ * by a secondary emulator, it will not be passed to that emulator.
+ */
+#define XEN_DMOP_map_io_range_to_ioreq_server 3
+#define XEN_DMOP_unmap_io_range_from_ioreq_server 4
+
+struct xen_dm_op_ioreq_server_range {
+	/* IN - server id */
+	ioservid_t id;
+	uint16_t pad;
+	/* IN - type of range */
+	uint32_t type;
+# define XEN_DMOP_IO_RANGE_PORT   0 /* I/O port range */
+# define XEN_DMOP_IO_RANGE_MEMORY 1 /* MMIO range */
+# define XEN_DMOP_IO_RANGE_PCI    2 /* PCI segment/bus/dev/func range */
+	/* IN - inclusive start and end of range */
+	uint64_aligned_t start, end;
+};
+
+#define XEN_DMOP_PCI_SBDF(s, b, d, f) \
+	((((s) & 0xffff) << 16) |  \
+	 (((b) & 0xff) << 8) |     \
+	 (((d) & 0x1f) << 3) |     \
+	 ((f) & 0x07))
+
+/*
+ * XEN_DMOP_set_ioreq_server_state: Enable or disable the IOREQ Server <id>
+ *
+ * The IOREQ Server will not be passed any emulation requests until it is
+ * in the enabled state.
+ * Note that the contents of the ioreq_pfn and bufioreq_fn (see
+ * XEN_DMOP_get_ioreq_server_info) are not meaningful until the IOREQ Server
+ * is in the enabled state.
+ */
+#define XEN_DMOP_set_ioreq_server_state 5
+
+struct xen_dm_op_set_ioreq_server_state {
+	/* IN - server id */
+	ioservid_t id;
+	/* IN - enabled? */
+	uint8_t enabled;
+	uint8_t pad;
+};
+
+/*
+ * XEN_DMOP_destroy_ioreq_server: Destroy the IOREQ Server <id>.
+ *
+ * Any registered I/O ranges will be automatically deregistered.
+ */
+#define XEN_DMOP_destroy_ioreq_server 6
+
+struct xen_dm_op_destroy_ioreq_server {
+	/* IN - server id */
+	ioservid_t id;
+	uint16_t pad;
+};
+
+/*
+ * XEN_DMOP_track_dirty_vram: Track modifications to the specified pfn
+ *                            range.
+ *
+ * NOTE: The bitmap passed back to the caller is passed in a
+ *       secondary buffer.
+ */
+#define XEN_DMOP_track_dirty_vram 7
+
+struct xen_dm_op_track_dirty_vram {
+	/* IN - number of pages to be tracked */
+	uint32_t nr;
+	uint32_t pad;
+	/* IN - first pfn to track */
+	uint64_aligned_t first_pfn;
+};
+
+/*
+ * XEN_DMOP_set_pci_intx_level: Set the logical level of one of a domain's
+ *                              PCI INTx pins.
+ */
+#define XEN_DMOP_set_pci_intx_level 8
+
+struct xen_dm_op_set_pci_intx_level {
+	/* IN - PCI INTx identification (domain:bus:device:intx) */
+	uint16_t domain;
+	uint8_t bus, device, intx;
+	/* IN - Level: 0 -> deasserted, 1 -> asserted */
+	uint8_t  level;
+};
+
+/*
+ * XEN_DMOP_set_isa_irq_level: Set the logical level of a one of a domain's
+ *                             ISA IRQ lines.
+ */
+#define XEN_DMOP_set_isa_irq_level 9
+
+struct xen_dm_op_set_isa_irq_level {
+	/* IN - ISA IRQ (0-15) */
+	uint8_t  isa_irq;
+	/* IN - Level: 0 -> deasserted, 1 -> asserted */
+	uint8_t  level;
+};
+
+/*
+ * XEN_DMOP_set_pci_link_route: Map a PCI INTx line to an IRQ line.
+ */
+#define XEN_DMOP_set_pci_link_route 10
+
+struct xen_dm_op_set_pci_link_route {
+	/* PCI INTx line (0-3) */
+	uint8_t  link;
+	/* ISA IRQ (1-15) or 0 -> disable link */
+	uint8_t  isa_irq;
+};
+
+/*
+ * XEN_DMOP_modified_memory: Notify that a set of pages were modified by
+ *                           an emulator.
+ *
+ * NOTE: In the event of a continuation, the @first_pfn is set to the
+ *       value of the pfn of the remaining set of pages and @nr reduced
+ *       to the size of the remaining set.
+ */
+#define XEN_DMOP_modified_memory 11
+
+struct xen_dm_op_modified_memory {
+	/* IN - number of contiguous pages modified */
+	uint32_t nr;
+	uint32_t pad;
+	/* IN - first pfn modified */
+	uint64_aligned_t first_pfn;
+};
+
+/*
+ * XEN_DMOP_set_mem_type: Notify that a region of memory is to be treated
+ *                        in a specific way. (See definition of
+ *                        hvmmem_type_t).
+ *
+ * NOTE: In the event of a continuation (return code -ERESTART), the
+ *       @first_pfn is set to the value of the pfn of the remaining
+ *       region and @nr reduced to the size of the remaining region.
+ */
+#define XEN_DMOP_set_mem_type 12
+
+struct xen_dm_op_set_mem_type {
+	/* IN - number of contiguous pages */
+	uint32_t nr;
+	/* IN - new hvmmem_type_t of region */
+	uint16_t mem_type;
+	uint16_t pad;
+	/* IN - first pfn in region */
+	uint64_aligned_t first_pfn;
+};
+
+/*
+ * XEN_DMOP_inject_event: Inject an event into a VCPU, which will
+ *                        get taken up when it is next scheduled.
+ *
+ * Note that the caller should know enough of the state of the CPU before
+ * injecting, to know what the effect of injecting the event will be.
+ */
+#define XEN_DMOP_inject_event 13
+
+struct xen_dm_op_inject_event {
+	/* IN - index of vCPU */
+	uint32_t vcpuid;
+	/* IN - interrupt vector */
+	uint8_t vector;
+	/* IN - event type (DMOP_EVENT_* ) */
+	uint8_t type;
+/* NB. This enumeration precisely matches hvm.h:X86_EVENTTYPE_* */
+# define XEN_DMOP_EVENT_ext_int    0 /* external interrupt */
+# define XEN_DMOP_EVENT_nmi        2 /* nmi */
+# define XEN_DMOP_EVENT_hw_exc     3 /* hardware exception */
+# define XEN_DMOP_EVENT_sw_int     4 /* software interrupt (CD nn) */
+# define XEN_DMOP_EVENT_pri_sw_exc 5 /* ICEBP (F1) */
+# define XEN_DMOP_EVENT_sw_exc     6 /* INT3 (CC), INTO (CE) */
+	/* IN - instruction length */
+	uint8_t insn_len;
+	uint8_t pad0;
+	/* IN - error code (or ~0 to skip) */
+	uint32_t error_code;
+	uint32_t pad1;
+	/* IN - CR2 for page faults */
+	uint64_aligned_t cr2;
+};
+
+/*
+ * XEN_DMOP_inject_msi: Inject an MSI for an emulated device.
+ */
+#define XEN_DMOP_inject_msi 14
+
+struct xen_dm_op_inject_msi {
+	/* IN - MSI data (lower 32 bits) */
+	uint32_t data;
+	uint32_t pad;
+	/* IN - MSI address (0xfeexxxxx) */
+	uint64_aligned_t addr;
+};
+
+/*
+ * XEN_DMOP_map_mem_type_to_ioreq_server : map or unmap the IOREQ Server <id>
+ *                                      to specific memory type <type>
+ *                                      for specific accesses <flags>
+ *
+ * For now, flags only accept the value of XEN_DMOP_IOREQ_MEM_ACCESS_WRITE,
+ * which means only write operations are to be forwarded to an ioreq server.
+ * Support for the emulation of read operations can be added when an ioreq
+ * server has such requirement in future.
+ */
+#define XEN_DMOP_map_mem_type_to_ioreq_server 15
+
+struct xen_dm_op_map_mem_type_to_ioreq_server {
+	ioservid_t id;      /* IN - ioreq server id */
+	uint16_t type;      /* IN - memory type */
+	uint32_t flags;     /* IN - types of accesses to be forwarded to the
+			       ioreq server. flags with 0 means to unmap the
+			       ioreq server */
+
+#define XEN_DMOP_IOREQ_MEM_ACCESS_READ (1u << 0)
+#define XEN_DMOP_IOREQ_MEM_ACCESS_WRITE (1u << 1)
+	uint64_t opaque;    /* IN/OUT - only used for hypercall continuation,
+			       has to be set to zero by the caller */
+};
+
+struct xen_dm_op {
+	uint32_t op;
+	uint32_t pad;
+	union {
+		struct xen_dm_op_create_ioreq_server create_ioreq_server;
+		struct xen_dm_op_get_ioreq_server_info get_ioreq_server_info;
+		struct xen_dm_op_ioreq_server_range map_io_range_to_ioreq_server;
+		struct xen_dm_op_ioreq_server_range unmap_io_range_from_ioreq_server;
+		struct xen_dm_op_set_ioreq_server_state set_ioreq_server_state;
+		struct xen_dm_op_destroy_ioreq_server destroy_ioreq_server;
+		struct xen_dm_op_track_dirty_vram track_dirty_vram;
+		struct xen_dm_op_set_pci_intx_level set_pci_intx_level;
+		struct xen_dm_op_set_isa_irq_level set_isa_irq_level;
+		struct xen_dm_op_set_pci_link_route set_pci_link_route;
+		struct xen_dm_op_modified_memory modified_memory;
+		struct xen_dm_op_set_mem_type set_mem_type;
+		struct xen_dm_op_inject_event inject_event;
+		struct xen_dm_op_inject_msi inject_msi;
+		struct xen_dm_op_map_mem_type_to_ioreq_server
+			map_mem_type_to_ioreq_server;
+	} u;
+};
+
 struct xen_dm_op_buf {
 	GUEST_HANDLE(void) h;
 	xen_ulong_t size;
 };
-DEFINE_GUEST_HANDLE_STRUCT(xen_dm_op_buf);
+typedef struct xen_dm_op_buf xen_dm_op_buf_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_dm_op_buf_t);
+
+/* ` enum neg_errnoval
+ * ` HYPERVISOR_dm_op(domid_t domid,
+ * `                  xen_dm_op_buf_t bufs[],
+ * `                  unsigned int nr_bufs)
+ * `
+ *
+ * @domid is the domain the hypercall operates on.
+ * @bufs points to an array of buffers where @bufs[0] contains a struct
+ * xen_dm_op, describing the specific device model operation and its
+ * parameters.
+ * @bufs[1..] may be referenced in the parameters for the purposes of
+ * passing extra information to or from the domain.
+ * @nr_bufs is the number of buffers in the @bufs array.
+ */
 
 #endif /* __XEN_PUBLIC_HVM_DM_OP_H__ */
+
+/*
+ * Local variables:
+ * mode: C
+ * c-file-style: "BSD"
+ * c-basic-offset: 4
+ * tab-width: 4
+ * indent-tabs-mode: nil
+ * End:
+ */
diff --git a/include/xen/interface/hvm/hvm_op.h b/include/xen/interface/hvm/hvm_op.h
index e1a4321..447bac1 100644
--- a/include/xen/interface/hvm/hvm_op.h
+++ b/include/xen/interface/hvm/hvm_op.h
@@ -16,23 +16,45 @@
  * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
  * DEALINGS IN THE SOFTWARE.
+ *
+ * Copyright (c) 2007, Keir Fraser
  */
 
 #ifndef __XEN_PUBLIC_HVM_HVM_OP_H__
 #define __XEN_PUBLIC_HVM_HVM_OP_H__
 
+#include "../xen.h"
+//#include "../trace.h"
 #include "../event_channel.h"
 
-/* Get/set subcommands: the second argument of the hypercall is a
- * pointer to a xen_hvm_param struct. */
+/* Get/set subcommands: extra argument == pointer to xen_hvm_param struct. */
 #define HVMOP_set_param           0
 #define HVMOP_get_param           1
 struct xen_hvm_param {
-    domid_t  domid;    /* IN */
-    uint32_t index;    /* IN */
-    uint64_t value;    /* IN/OUT */
+	domid_t  domid;    /* IN */
+	uint32_t index;    /* IN */
+	uint64_t value;    /* IN/OUT */
 };
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_param);
+typedef struct xen_hvm_param xen_hvm_param_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_param_t);
+
+/* Flushes all VCPU TLBs: @arg must be NULL. */
+#define HVMOP_flush_tlbs          5
+
+typedef enum {
+	HVMMEM_ram_rw,             /* Normal read/write guest RAM */
+	HVMMEM_ram_ro,             /* Read-only; writes are discarded */
+	HVMMEM_mmio_dm,            /* Reads and write go to the device model */
+	HVMMEM_unused,             /* Placeholder; setting memory to this type
+				      will fail for code after 4.7.0 */
+	HVMMEM_ioreq_server        /* Memory type claimed by an ioreq server; type
+				      changes to this value are only allowed after
+				      an ioreq server has claimed its ownership.
+				      Only pages with HVMMEM_ram_rw are allowed to
+				      change to this type; conversely, pages with
+				      this type are only allowed to be changed back
+				      to HVMMEM_ram_rw. */
+} hvmmem_type_t;
 
 #define HVMOP_set_pci_intx_level  2
 struct xen_hvm_set_pci_intx_level {
@@ -101,207 +123,194 @@ struct xen_hvm_set_mem_type {
 };
 
 /* Hint from PV drivers for pagetable destruction. */
-#define HVMOP_pagetable_dying       9
+#define HVMOP_pagetable_dying        9
 struct xen_hvm_pagetable_dying {
-    /* Domain with a pagetable about to be destroyed. */
-    domid_t  domid;
-    /* guest physical address of the toplevel pagetable dying */
-    aligned_u64 gpa;
+	/* Domain with a pagetable about to be destroyed. */
+	domid_t  domid;
+	uint16_t pad[3]; /* align next field on 8-byte boundary */
+	/* guest physical address of the toplevel pagetable dying */
+	uint64_t gpa;
 };
 typedef struct xen_hvm_pagetable_dying xen_hvm_pagetable_dying_t;
 DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_pagetable_dying_t);
 
-/* MSI injection for emulated devices */
-#define HVMOP_inject_msi         16
-struct xen_hvm_inject_msi {
-    /* Domain to be injected */
-    domid_t   domid;
-    /* Data -- lower 32 bits */
-    uint32_t  data;
-    /* Address (0xfeexxxxx) */
-    uint64_t  addr;
-};
-typedef struct xen_hvm_inject_msi xen_hvm_inject_msi_t;
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_inject_msi_t);
-
-enum hvmmem_type_t {
-    HVMMEM_ram_rw,             /* Normal read/write guest RAM */
-    HVMMEM_ram_ro,             /* Read-only; writes are discarded */
-    HVMMEM_mmio_dm,            /* Reads and write go to the device model */
-    HVMMEM_mmio_write_dm       /* Read-only; writes go to the device model */
+/* Get the current Xen time, in nanoseconds since system boot. */
+#define HVMOP_get_time              10
+struct xen_hvm_get_time {
+	uint64_t now;      /* OUT */
 };
+typedef struct xen_hvm_get_time xen_hvm_get_time_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_get_time_t);
 
-#define HVMOP_set_mem_type    8
-/* Notify that a region of memory is to be treated in a specific way. */
-struct xen_hvm_set_mem_type {
-	/* Domain to be updated. */
-	domid_t domid;
-	/* Memory type */
-	uint16_t hvmmem_type;
-	/* Number of pages. */
-	uint32_t nr;
-	/* First pfn. */
-	uint64_t first_pfn;
-};
-typedef struct xen_hvm_set_mem_type xen_hvm_set_mem_type_t;
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_set_mem_type_t);
+//#define HVMOP_xentrace              11
+//struct xen_hvm_xentrace {
+//    uint16_t event, extra_bytes;
+//    uint8_t extra[TRACE_EXTRA_MAX * sizeof(uint32_t)];
+//};
+//typedef struct xen_hvm_xentrace xen_hvm_xentrace_t;
+//DEFINE_XEN_GUEST_HANDLE(xen_hvm_xentrace_t);
+
+/* Following tools-only interfaces may change in future. */
+#if defined(__XEN__) || defined(__XEN_TOOLS__)
+
+/* Deprecated by XENMEM_access_op_set_access */
+#define HVMOP_set_mem_access        12
 
+/* Deprecated by XENMEM_access_op_get_access */
+#define HVMOP_get_mem_access        13
+
+#endif /* defined(__XEN__) || defined(__XEN_TOOLS__) */
 
 #define HVMOP_get_mem_type    15
 /* Return hvmmem_type_t for the specified pfn. */
 struct xen_hvm_get_mem_type {
-    /* Domain to be queried. */
-    domid_t domid;
-    /* OUT variable. */
-    uint16_t mem_type;
-    uint16_t pad[2]; /* align next field on 8-byte boundary */
-    /* IN variable. */
-    uint64_t pfn;
-};
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_get_mem_type);
-
-#define HVMOP_vgt_wp_pages         27  /* writeprotection to guest pages */
-#define MAX_WP_BATCH_PAGES         128
-struct xen_hvm_vgt_wp_pages {
-	uint16_t domid;
-	uint16_t set;            /* 1: set WP, 0: remove WP */
-	uint16_t nr_pages;
-	unsigned long  wp_pages[MAX_WP_BATCH_PAGES];
+	/* Domain to be queried. */
+	domid_t domid;
+	/* OUT variable. */
+	uint16_t mem_type;
+	uint16_t pad[2]; /* align next field on 8-byte boundary */
+	/* IN variable. */
+	uint64_t pfn;
 };
-typedef struct xen_hvm_vgt_wp_pages xen_hvm_vgt_wp_pages_t;
+typedef struct xen_hvm_get_mem_type xen_hvm_get_mem_type_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_get_mem_type_t);
+
+/* Following tools-only interfaces may change in future. */
+#if defined(__XEN__) || defined(__XEN_TOOLS__)
 
 /*
- * IOREQ Servers
- *
- * The interface between an I/O emulator an Xen is called an IOREQ Server.
- * A domain supports a single 'legacy' IOREQ Server which is instantiated if
- * parameter...
- *
- * HVM_PARAM_IOREQ_PFN is read (to get the gmfn containing the synchronous
- * ioreq structures), or...
- * HVM_PARAM_BUFIOREQ_PFN is read (to get the gmfn containing the buffered
- * ioreq ring), or...
- * HVM_PARAM_BUFIOREQ_EVTCHN is read (to get the event channel that Xen uses
- * to request buffered I/O emulation).
- *
- * The following hypercalls facilitate the creation of IOREQ Servers for
- * 'secondary' emulators which are invoked to implement port I/O, memory, or
- * PCI config space ranges which they explicitly register.
+ * Definitions relating to DMOP_create_ioreq_server. (Defined here for
+ * backwards compatibility).
  */
-typedef uint16_t ioservid_t;
 
+#define HVM_IOREQSRV_BUFIOREQ_OFF    0
+#define HVM_IOREQSRV_BUFIOREQ_LEGACY 1
 /*
- * HVMOP_create_ioreq_server: Instantiate a new IOREQ Server for a secondary
- *                            emulator servicing domain <domid>.
- *
- * The <id> handed back is unique for <domid>. If <handle_bufioreq> is zero
- * the buffered ioreq ring will not be allocated and hence all emulation
- * requestes to this server will be synchronous.
+ * Use this when read_pointer gets updated atomically and
+ * the pointer pair gets read atomically:
  */
-#define HVMOP_create_ioreq_server 17
-struct xen_hvm_create_ioreq_server {
-    domid_t domid;           /* IN - domain to be serviced */
-    uint8_t handle_bufioreq; /* IN - should server handle buffered ioreqs */
-    ioservid_t id;           /* OUT - server id */
-};
-typedef struct xen_hvm_create_ioreq_server xen_hvm_create_ioreq_server_t;
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_create_ioreq_server_t);
+#define HVM_IOREQSRV_BUFIOREQ_ATOMIC 2
+
+#endif /* defined(__XEN__) || defined(__XEN_TOOLS__) */
+
+#if defined(__i386__) || defined(__x86_64__)
 
 /*
- * HVMOP_get_ioreq_server_info: Get all the information necessary to access
- *                              IOREQ Server <id>.
- *
- * The emulator needs to map the synchronous ioreq structures and buffered
- * ioreq ring (if it exists) that Xen uses to request emulation. These are
- * hosted in domain <domid>'s gmfns <ioreq_pfn> and <bufioreq_pfn>
- * respectively. In addition, if the IOREQ Server is handling buffered
- * emulation requests, the emulator needs to bind to event channel
- * <bufioreq_port> to listen for them. (The event channels used for
- * synchronous emulation requests are specified in the per-CPU ioreq
- * structures in <ioreq_pfn>).
- * If the IOREQ Server is not handling buffered emulation requests then the
- * values handed back in <bufioreq_pfn> and <bufioreq_port> will both be 0.
+ * HVMOP_set_evtchn_upcall_vector: Set a <vector> that should be used for event
+ *                                 channel upcalls on the specified <vcpu>. If set,
+ *                                 this vector will be used in preference to the
+ *                                 domain global callback via (see
+ *                                 HVM_PARAM_CALLBACK_IRQ).
  */
-#define HVMOP_get_ioreq_server_info 18
-struct xen_hvm_get_ioreq_server_info {
-    domid_t domid;                 /* IN - domain to be serviced */
-    ioservid_t id;                 /* IN - server id */
-    evtchn_port_t bufioreq_port;   /* OUT - buffered ioreq port */
-    uint64_t ioreq_pfn;    /* OUT - sync ioreq pfn */
-    uint64_t bufioreq_pfn; /* OUT - buffered ioreq pfn */
+#define HVMOP_set_evtchn_upcall_vector 23
+struct xen_hvm_evtchn_upcall_vector {
+	uint32_t vcpu;
+	uint8_t vector;
 };
-typedef struct xen_hvm_get_ioreq_server_info xen_hvm_get_ioreq_server_info_t;
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_get_ioreq_server_info_t);
+typedef struct xen_hvm_evtchn_upcall_vector xen_hvm_evtchn_upcall_vector_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_evtchn_upcall_vector_t);
 
-/*
- * HVM_map_io_range_to_ioreq_server: Register an I/O range of domain <domid>
- *                                   for emulation by the client of IOREQ
- *                                   Server <id>
- * HVM_unmap_io_range_from_ioreq_server: Deregister an I/O range of <domid>
- *                                       for emulation by the client of IOREQ
- *                                       Server <id>
- *
- * There are three types of I/O that can be emulated: port I/O, memory accesses
- * and PCI config space accesses. The <type> field denotes which type of range
- * the <start> and <end> (inclusive) fields are specifying.
- * PCI config space ranges are specified by segment/bus/device/function values
- * which should be encoded using the HVMOP_PCI_SBDF helper macro below.
- *
- * NOTE: unless an emulation request falls entirely within a range mapped
- * by a secondary emulator, it will not be passed to that emulator.
- */
-#define HVMOP_map_io_range_to_ioreq_server 19
-#define HVMOP_unmap_io_range_from_ioreq_server 20
-struct xen_hvm_io_range {
-    domid_t domid;               /* IN - domain to be serviced */
-    ioservid_t id;               /* IN - server id */
-    uint32_t type;               /* IN - type of range */
-# define HVMOP_IO_RANGE_PORT   0 /* I/O port range */
-# define HVMOP_IO_RANGE_MEMORY 1 /* MMIO range */
-# define HVMOP_IO_RANGE_PCI    2 /* PCI segment/bus/dev/func range */
-    uint64_t start, end; /* IN - inclusive start and end of range */
+#endif /* defined(__i386__) || defined(__x86_64__) */
+
+#define HVMOP_guest_request_vm_event 24
+
+/* HVMOP_altp2m: perform altp2m state operations */
+#define HVMOP_altp2m 25
+
+#define HVMOP_ALTP2M_INTERFACE_VERSION 0x00000001
+
+struct xen_hvm_altp2m_domain_state {
+	/* IN or OUT variable on/off */
+	uint8_t state;
 };
-typedef struct xen_hvm_io_range xen_hvm_io_range_t;
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_io_range_t);
+typedef struct xen_hvm_altp2m_domain_state xen_hvm_altp2m_domain_state_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_altp2m_domain_state_t);
 
-#define HVMOP_PCI_SBDF(s,b,d,f)                 \
-       ((((s) & 0xffff) << 16) |                   \
-        (((b) & 0xff) << 8) |                      \
-        (((d) & 0x1f) << 3) |                      \
-        ((f) & 0x07))
+struct xen_hvm_altp2m_vcpu_enable_notify {
+	uint32_t vcpu_id;
+	uint32_t pad;
+	/* #VE info area gfn */
+	uint64_t gfn;
+};
+typedef struct xen_hvm_altp2m_vcpu_enable_notify xen_hvm_altp2m_vcpu_enable_notify_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_altp2m_vcpu_enable_notify_t);
 
-/*
- * HVMOP_destroy_ioreq_server: Destroy the IOREQ Server <id> servicing domain
- *                             <domid>.
- *
- * Any registered I/O ranges will be automatically deregistered.
- */
-#define HVMOP_destroy_ioreq_server 21
-struct xen_hvm_destroy_ioreq_server {
-    domid_t domid; /* IN - domain to be serviced */
-    ioservid_t id; /* IN - server id */
+struct xen_hvm_altp2m_view {
+	/* IN/OUT variable */
+	uint16_t view;
+	/* Create view only: default access type
+	 * NOTE: currently ignored */
+	uint16_t hvmmem_default_access; /* xenmem_access_t */
 };
-typedef struct xen_hvm_destroy_ioreq_server xen_hvm_destroy_ioreq_server_t;
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_destroy_ioreq_server_t);
+typedef struct xen_hvm_altp2m_view xen_hvm_altp2m_view_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_altp2m_view_t);
 
+struct xen_hvm_altp2m_set_mem_access {
+	/* view */
+	uint16_t view;
+	/* Memory type */
+	uint16_t hvmmem_access; /* xenmem_access_t */
+	uint32_t pad;
+	/* gfn */
+	uint64_t gfn;
+};
+typedef struct xen_hvm_altp2m_set_mem_access xen_hvm_altp2m_set_mem_access_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_altp2m_set_mem_access_t);
 
-/*
- * HVMOP_set_ioreq_server_state: Enable or disable the IOREQ Server <id> servicing
- *                               domain <domid>.
- *
- * The IOREQ Server will not be passed any emulation requests until it is in the
- * enabled state.
- * Note that the contents of the ioreq_pfn and bufioreq_fn (see
- * HVMOP_get_ioreq_server_info) are not meaningful until the IOREQ Server is in
- * the enabled state.
- */
-#define HVMOP_set_ioreq_server_state 22
-struct xen_hvm_set_ioreq_server_state {
-    domid_t domid;   /* IN - domain to be serviced */
-    ioservid_t id;   /* IN - server id */
-    uint8_t enabled; /* IN - enabled? */
+struct xen_hvm_altp2m_change_gfn {
+	/* view */
+	uint16_t view;
+	uint16_t pad1;
+	uint32_t pad2;
+	/* old gfn */
+	uint64_t old_gfn;
+	/* new gfn, INVALID_GFN (~0UL) means revert */
+	uint64_t new_gfn;
+};
+typedef struct xen_hvm_altp2m_change_gfn xen_hvm_altp2m_change_gfn_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_altp2m_change_gfn_t);
+
+struct xen_hvm_altp2m_op {
+	uint32_t version;   /* HVMOP_ALTP2M_INTERFACE_VERSION */
+	uint32_t cmd;
+/* Get/set the altp2m state for a domain */
+#define HVMOP_altp2m_get_domain_state     1
+#define HVMOP_altp2m_set_domain_state     2
+/* Set the current VCPU to receive altp2m event notifications */
+#define HVMOP_altp2m_vcpu_enable_notify   3
+/* Create a new view */
+#define HVMOP_altp2m_create_p2m           4
+/* Destroy a view */
+#define HVMOP_altp2m_destroy_p2m          5
+/* Switch view for an entire domain */
+#define HVMOP_altp2m_switch_p2m           6
+/* Notify that a page of memory is to have specific access types */
+#define HVMOP_altp2m_set_mem_access       7
+/* Change a p2m entry to have a different gfn->mfn mapping */
+#define HVMOP_altp2m_change_gfn           8
+	domid_t domain;
+	uint16_t pad1;
+	uint32_t pad2;
+	union {
+		struct xen_hvm_altp2m_domain_state       domain_state;
+		struct xen_hvm_altp2m_vcpu_enable_notify enable_notify;
+		struct xen_hvm_altp2m_view               view;
+		struct xen_hvm_altp2m_set_mem_access     set_mem_access;
+		struct xen_hvm_altp2m_change_gfn         change_gfn;
+		uint8_t pad[64];
+    } u;
 };
-typedef struct xen_hvm_set_ioreq_server_state xen_hvm_set_ioreq_server_state_t;
-DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_set_ioreq_server_state_t);
+typedef struct xen_hvm_altp2m_op xen_hvm_altp2m_op_t;
+DEFINE_GUEST_HANDLE_STRUCT(xen_hvm_altp2m_op_t);
 
 #endif /* __XEN_PUBLIC_HVM_HVM_OP_H__ */
+
+/*
+ * Local variables:
+ * mode: C
+ * c-file-style: "BSD"
+ * c-basic-offset: 4
+ * tab-width: 4
+ * indent-tabs-mode: nil
+ * End:
+ */
diff --git a/include/xen/interface/hvm/ioreq.h b/include/xen/interface/hvm/ioreq.h
index 6bbf4e4..a9a3fd5 100644
--- a/include/xen/interface/hvm/ioreq.h
+++ b/include/xen/interface/hvm/ioreq.h
@@ -1,17 +1,26 @@
 /*
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
+ * ioreq.h: I/O request definitions for device models
+ * Copyright (c) 2004, Intel Corporation.
  *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to
+ * deal in the Software without restriction, including without limitation the
+ * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+ * sell copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
  *
- * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc.,
- * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
+ * DEALINGS IN THE SOFTWARE.
  */
+
 #ifndef _IOREQ_H_
 #define _IOREQ_H_
 
@@ -32,66 +41,63 @@
 /*
  * VMExit dispatcher should cooperate with instruction decoder to
  * prepare this structure and notify service OS and DM by sending
- * virq
+ * virq.
+ *
+ * For I/O type IOREQ_TYPE_PCI_CONFIG, the physical address is formatted
+ * as follows:
+ *
+ * 63....48|47..40|39..35|34..32|31........0
+ * SEGMENT |BUS   |DEV   |FN    |OFFSET
  */
 struct ioreq {
-    uint64_t addr;          /* physical address */
-    uint64_t data;          /* data (or paddr of data) */
-    uint32_t count;         /* for rep prefixes */
-    uint32_t size;          /* size in bytes */
-    uint32_t vp_eport;      /* evtchn for notifications to/from device model */
-    uint16_t _pad0;
-    uint8_t state:4;
-    uint8_t data_is_ptr:1;  /* if 1, data above is the guest paddr
-                             * of the real data to use. */
-    uint8_t dir:1;          /* 1=read, 0=write */
-    uint8_t df:1;
-    uint8_t _pad1:1;
-    uint8_t type;           /* I/O type */
+	uint64_t addr;          /* physical address */
+	uint64_t data;          /* data (or paddr of data) */
+	uint32_t count;         /* for rep prefixes */
+	uint32_t size;          /* size in bytes */
+	uint32_t vp_eport;      /* evtchn for notifications to/from device model */
+	uint16_t _pad0;
+	uint8_t state:4;
+	uint8_t data_is_ptr:1;  /* if 1, data above is the guest paddr
+				 * of the real data to use. */
+	uint8_t dir:1;          /* 1=read, 0=write */
+	uint8_t df:1;
+	uint8_t _pad1:1;
+	uint8_t type;           /* I/O type */
 };
 typedef struct ioreq ioreq_t;
 
 struct shared_iopage {
-    struct ioreq vcpu_ioreq[1];
+	struct ioreq vcpu_ioreq[1];
 };
 typedef struct shared_iopage shared_iopage_t;
 
 struct buf_ioreq {
-    uint8_t  type;   /* I/O type                    */
-    uint8_t  pad:1;
-    uint8_t  dir:1;  /* 1=read, 0=write             */
-    uint8_t  size:2; /* 0=>1, 1=>2, 2=>4, 3=>8. If 8, use two buf_ioreqs */
-    uint32_t addr:20;/* physical address            */
-    uint32_t data;   /* data                        */
+	uint8_t  type;   /* I/O type                    */
+	uint8_t  pad:1;
+	uint8_t  dir:1;  /* 1=read, 0=write             */
+	uint8_t  size:2; /* 0=>1, 1=>2, 2=>4, 3=>8. If 8, use two buf_ioreqs */
+	uint32_t addr:20;/* physical address            */
+	uint32_t data;   /* data                        */
 };
 typedef struct buf_ioreq buf_ioreq_t;
 
 #define IOREQ_BUFFER_SLOT_NUM     511 /* 8 bytes each, plus 2 4-byte indexes */
 struct buffered_iopage {
-    unsigned int read_pointer;
-    unsigned int write_pointer;
-    buf_ioreq_t buf_ioreq[IOREQ_BUFFER_SLOT_NUM];
+#ifdef __XEN__
+	union bufioreq_pointers {
+		struct {
+#endif
+			uint32_t read_pointer;
+			uint32_t write_pointer;
+#ifdef __XEN__
+		};
+		uint64_t full;
+	} ptrs;
+#endif
+	buf_ioreq_t buf_ioreq[IOREQ_BUFFER_SLOT_NUM];
 }; /* NB. Size of this structure must be no greater than one page. */
 typedef struct buffered_iopage buffered_iopage_t;
 
-#if defined(__ia64__)
-struct pio_buffer {
-    uint32_t page_offset;
-    uint32_t pointer;
-    uint32_t data_end;
-    uint32_t buf_size;
-    void *opaque;
-};
-
-#define PIO_BUFFER_IDE_PRIMARY   0 /* I/O port = 0x1F0 */
-#define PIO_BUFFER_IDE_SECONDARY 1 /* I/O port = 0x170 */
-#define PIO_BUFFER_ENTRY_NUM     2
-struct buffered_piopage {
-    struct pio_buffer pio[PIO_BUFFER_ENTRY_NUM];
-    uint8_t buffer[1];
-};
-#endif /* defined(__ia64__) */
-
 /*
  * ACPI Control/Event register locations. Location is controlled by a
  * version number in HVM_PARAM_ACPI_IOPORTS_LOCATION.
@@ -124,7 +130,7 @@ struct buffered_piopage {
 /*
  * Local variables:
  * mode: C
- * c-set-style: "BSD"
+ * c-file-style: "BSD"
  * c-basic-offset: 4
  * tab-width: 4
  * indent-tabs-mode: nil
-- 
2.7.4

